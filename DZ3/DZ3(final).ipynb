{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DZ3zd1(pochti) (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiJR3dodE6SH",
        "colab_type": "text"
      },
      "source": [
        "Задание 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yFW54DLeZMm",
        "colab_type": "code",
        "outputId": "ecabaa5a-4e47-4d57-b930-064f892c5bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 11:58:30--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt’\n",
            "\n",
            "\rsents_with_mistakes   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 11:58:30 (3.08 MB/s) - ‘sents_with_mistakes.txt’ saved [123167/123167]\n",
            "\n",
            "--2019-11-22 11:58:32--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt’\n",
            "\n",
            "correct_sents.txt   100%[===================>] 117.84K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 11:58:32 (2.98 MB/s) - ‘correct_sents.txt’ saved [120672/120672]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QPIWfAqE0gA",
        "colab": {}
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2C34kPqD-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nJykW-jeH7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBaLM95uiK1V",
        "colab_type": "code",
        "outputId": "42fb6b87-1d98-4c66-cfeb-e5b151ddd873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHUyta_ALSMb",
        "colab_type": "code",
        "outputId": "422bf319-9dd0-4d93-9d9d-6da007a442b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 11:59:55--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1181403 (1.1M) [text/plain]\n",
            "Saving to: ‘corpus_500.txt.1’\n",
            "\n",
            "\rcorpus_500.txt.1      0%[                    ]       0  --.-KB/s               \rcorpus_500.txt.1    100%[===================>]   1.13M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-11-22 11:59:55 (12.9 MB/s) - ‘corpus_500.txt.1’ saved [1181403/1181403]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RMMWwrDeH9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2EHLBfEeH9K",
        "colab_type": "code",
        "outputId": "0699bef1-997b-431c-d52e-d9af4574cda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_500.txt').read().split():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "\n",
        "print(corpus[:10])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['text'], ['вице-премьер'], ['по'], ['социальным'], ['вопросам'], ['татьяна'], ['голикова'], ['рассказала'], ['в'], ['каких']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ko8ui2ceH9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_mistaken(word, vocab):\n",
        "\n",
        "    if word in vocab:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOu_rBq8eH95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Pjtzu0eH-I",
        "colab_type": "code",
        "outputId": "ccd1c5dc-c501-4460-d7b0-790c2a6af611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "WORDS.most_common(10)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('в', 4263),\n",
              " ('и', 1999),\n",
              " ('на', 1757),\n",
              " ('что', 1236),\n",
              " ('с', 990),\n",
              " ('по', 850),\n",
              " ('не', 739),\n",
              " ('он', 425),\n",
              " ('из', 406),\n",
              " ('этом', 405)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF1sFv10eH-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# фунцкия расчета вероятности слова\n",
        "N = sum(WORDS.values())\n",
        "def Pos(word, N=N): \n",
        "    \"Вычисляем вероятность слова\"\n",
        "    return WORDS[word] / N\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLaq92IUO5C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict2 = {}\n",
        "for key in WORDS:\n",
        "  splits     =  [(key[:i], key[i:])    for i in range(len(key) + 1)]\n",
        "  deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "  splits2     = [(key[:i], key[i:])    for i in range(len(deletes1))]\n",
        "  deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "  deletes = deletes1 + deletes2\n",
        "  for element in deletes: \n",
        "    dict2[element] = key\n",
        " \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ucHyDYca2ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare(word):\n",
        "  cand = []\n",
        "  if word in WORDS:\n",
        "    return word\n",
        "  elif word in dict2:\n",
        "    cand.append(dict2[word])\n",
        "  else:\n",
        "    splits     =  [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    for delts in deletes:\n",
        "      if delts in WORDS:\n",
        "        cand.append(delts)\n",
        "      elif delts in dict2:\n",
        "        cand.append(dict2[delts])\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  if cand:\n",
        "    return max(cand, key=Pos)\n",
        "  else:\n",
        "    return 'Нельзя исправить'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AhvGsi3c9ta",
        "colab_type": "code",
        "outputId": "10970e1c-cd17-4b4e-86ce-6228720844ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "compare('онор')"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 100 µs, sys: 14 µs, total: 114 µs\n",
            "Wall time: 120 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'он'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJLVOZ7eH-t",
        "colab_type": "code",
        "outputId": "c3c6c0ec-c03a-4567-f247-50e770bdd574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%%time\n",
        "print(compare('мш,ина'))\n",
        "print(compare('ручноой'))\n",
        "print(compare('речнйо'))\n",
        "print(compare('седня'))\n",
        "print(compare('оочень'))\n"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "машина\n",
            "ручной\n",
            "речного\n",
            "сегодня\n",
            "очень\n",
            "CPU times: user 713 µs, sys: 0 ns, total: 713 µs\n",
            "Wall time: 669 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi0QTU0teH_2",
        "colab_type": "code",
        "outputId": "a29ca204-8e77-4418-a055-5c3c5f190931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], compare(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "        print(i)\n",
        "        "
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC3274axeH_8",
        "colab_type": "code",
        "outputId": "77e88c8d-f6d6-4c0f-f1dd-0cc2e9a3465c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5247752247752248\n",
            "0.3461243284727552\n",
            "0.44848972091420697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqaXTzTxW3E5",
        "colab_type": "text"
      },
      "source": [
        "Качество стало хуже. Достаточно высокий показатель неправильно исправленных правильных слов. Может быть, проблема кроется в том, что в выбранном корпусе много коротких слов, которые сокращаются до последовательностьй в один-два знака, которые затем выискиваются в словаре ошибок. При этом выдаётся самое частотное слово. Выше есть пример с 'онор', которое исправляется на 'он', вместо 'оно', просто потому, что слово 'он' более частотное. С этим, наверное, можно бороться, назначив приоритет слову с одним удалением. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvX_eauW3u7",
        "colab_type": "text"
      },
      "source": [
        "Задание 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T92But3Aeby4",
        "colab_type": "code",
        "outputId": "2e0bdeda-123e-4805-a335-edaa2adef648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "corpus2 = []\n",
        "for text in open('corpus_500.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus2 += norm_sents\n",
        "\n",
        "print(corpus2[:10])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['text'], ['вице-премьер', 'по', 'социальным', 'вопросам', 'татьяна', 'голикова', 'рассказала', 'в', 'каких', 'регионах', 'россии', 'зафиксирована', 'наиболее', 'высокая', 'смертность', 'от', 'рака', 'сообщает', 'риа', 'новости'], ['по', 'словам', 'голиковой', 'чаще', 'всего', 'онкологические', 'заболевания', 'становились', 'причиной', 'смерти', 'в', 'псковской', 'тверской', 'тульской', 'и', 'орловской', 'областях', 'а', 'также', 'в', 'севастополе'], ['вице-премьер', 'напомнила', 'что', 'главные', 'факторы', 'смертности', 'в', 'россии', 'рак', 'и', 'болезни', 'системы', 'кровообращения'], ['в', 'начале', 'года', 'стало', 'известно', 'что', 'смертность', 'от', 'онкологических', 'заболеваний', 'среди', 'россиян', 'снизилась', 'впервые', 'за', 'три', 'года'], ['по', 'данным', 'росстата', 'в', '2017', 'году', 'от', 'рака', 'умерли', '289', 'тысяч', 'человек'], ['это', 'на', '3,5', 'процента', 'меньше', 'чем', 'годом', 'ранее'], ['австрийские', 'правоохранительные', 'органы', 'не', 'представили', 'доказательств', 'нарушения', 'российскими', 'биатлонистами', 'антидопинговых', 'правил'], ['об', 'этом', 'сообщил', 'посол', 'россии', 'в', 'вене', 'дмитрий', 'любинский', 'по', 'итогам', 'встречи', 'уполномоченного', 'адвоката', 'дипмиссии', 'с', 'представителями', 'прокуратуры', 'страны', 'передает', 'тасс'], ['действует', 'презумпция', 'невиновности']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtpU8_iLW9mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer2(tokens, n=2):\n",
        "    ngrams2 = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams2.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams2\n",
        "def ngrammer3(tokens, n=3):\n",
        "    ngrams3 = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams3.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHqVYa5YMO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_sents = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVZF4DqGYi3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams= Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_sents:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer2(sentence))\n",
        "    trigrams.update(ngrammer3(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FGex52eZEe6",
        "colab_type": "code",
        "outputId": "5e8362f8-ca07-4ee3-cb97-79f3b145a084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "trigrams.most_common(10)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start> <start> в', 710),\n",
              " ('<start> <start> по', 323),\n",
              " ('<start> <start> об', 296),\n",
              " ('<start> об этом', 290),\n",
              " ('<start> <start> он', 132),\n",
              " ('<start> <start> на', 132),\n",
              " ('об этом сообщает', 125),\n",
              " ('<start> <start> ранее', 102),\n",
              " ('<start> по его', 75),\n",
              " ('<start> по словам', 73)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPMu0Z8XZ3nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict2_unigrams = {}\n",
        "for word in unigrams:\n",
        "  splits     =  [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "  deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "  splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "  deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "  deletes = deletes1 + deletes2\n",
        "  for element in deletes: \n",
        "    dict2_unigrams[element] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvPv5_xLaC7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict2_bigrams = {}\n",
        "for word in bigrams:\n",
        "  splits     =  [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "  deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "  splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "  deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "  deletes = deletes1 + deletes2\n",
        "  for element in deletes: \n",
        "    dict2_bigrams[element] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0OPJcI0aDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict2_trigrams = {}\n",
        "for word in trigrams:\n",
        "  splits     =  [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "  deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "  splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "  deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "  deletes = deletes1 + deletes2\n",
        "  for element in deletes: \n",
        "    dict2_trigrams[element] = word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gd3iwe8IDUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def poser(delts):\n",
        "  delt = str(delts.split()[:-1])\n",
        "  if delt in WORDS:\n",
        "    return Pos(delt)\n",
        "  elif delt in dict2:\n",
        "    delts_pos = unigrams[delt]/len(unigrams)\n",
        "    bigram = \" \".join(delt)\n",
        "    if WORDS[delts.split()[0]]:\n",
        "      bigram_pos = bigrams[bigram]/unigrams[delts.split()[0]]\n",
        "      trigram = bigram + \" \" + dict2[delt]\n",
        "    if bigrams[bigram]:\n",
        "      trigram_pos = trigrams[trigram]/bigrams[bigram]\n",
        "      context_pos = delts_pos*(1 + trigram_pos/bigram_pos)\n",
        "      return context_pos\n",
        "    else:\n",
        "      context_pos = delts_pos\n",
        "    return context_pos\n",
        "  else:\n",
        "    return 0\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbijjx8osvYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_grams(word):\n",
        "  cand = []\n",
        "  if word in WORDS:\n",
        "    return word\n",
        "  elif word in dict2:\n",
        "    cand.append(dict2[word])\n",
        "  elif word in dict2_bigrams:\n",
        "    cand.append(dict2_bigrams[word])\n",
        "  elif word in dict2_trigrams:\n",
        "    cand.append(dict2_trigrams[word])  \n",
        "  else:\n",
        "    splits     =  [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes3 = deletes1 + deletes2\n",
        "\n",
        "    word1 = str(word.split()[-1])\n",
        "    splits3     =  [(word1[:i], word1[i:])    for i in range(len(word1) + 1)]\n",
        "    deletes4    = [L + R[1:]               for L, R in splits3 if R]\n",
        "    splits4     = [(word1[:i], word1[i:])    for i in range(len(deletes4))]\n",
        "    deletes5    = [L + R[2:]               for L, R in splits4 if R]\n",
        "    deletes6 = deletes3 + deletes4 + deletes5\n",
        "\n",
        "    for delts in deletes6:\n",
        "      if delts in WORDS:\n",
        "        cand.append(delts)\n",
        "      elif delts in dict2:\n",
        "        cand.append(dict2[delts])\n",
        "      elif delts in dict2_bigrams:\n",
        "        cand.append(dict2_bigrams[delts])\n",
        "      elif delts in dict2_trigrams:\n",
        "        cand.append(dict2_trigrams[delts])\n",
        "\n",
        "\n",
        "  if cand:\n",
        "    return max(cand, key=poser)\n",
        "  else:\n",
        "    return 'Нельзя исправить'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66LyTNfc5xJ",
        "colab_type": "code",
        "outputId": "56026825-09c2-462d-e713-89391d86888b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(compare('какт'))\n",
        "print(correct_grams('какт'))\n",
        "print(correct_grams('ывлоаыаоыола'))"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "как\n",
            "как от\n",
            "Нельзя исправить\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "98fa5813-b70b-4782-d4b8-2588c6125e6d",
        "id": "wRYUTJ-5nm7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "%%time\n",
        "print(correct_grams('мшина'))\n",
        "print(correct_grams('яптницу 14'))\n",
        "print(correct_grams('сосылкой н'))\n",
        "print(correct_grams('об этам сообщает'))\n",
        "print(correct_grams('общает'))\n",
        "print(correct_grams('как ообщает'))\n"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "машина\n",
            "пятницу 14\n",
            "ссылкой на\n",
            "об этом сообщает\n",
            "общается\n",
            "как сообщает\n",
            "CPU times: user 2.58 ms, sys: 79 µs, total: 2.66 ms\n",
            "Wall time: 7.23 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "336da0c2-27d9-4be4-fff8-a7dee7972fca",
        "id": "coNn5TZWot2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], correct_grams(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "        print(i)\n",
        "        "
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8f2919b0-5f93-46d3-c5af-fe2fd9b3568a",
        "id": "ACN9l5F1ox7J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.509090909090909\n",
            "0.3215656178050652\n",
            "0.4628459859882853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvoWqmEGqkCw",
        "colab_type": "text"
      },
      "source": [
        "С нграмной моделью упало количество правильно исправленных слов и возросло количество неправильно исправленных, что не есть хорошо. Можно сделать вывод, что качество упало. Проблема может быть в том, что алгоритм с большей вероятностью выбирает нграмм, в составе которого есть нужное слово (потому что урезанные на несколько знаков нграммы могут добавлятся сразу на нескольких этапах). К тому же, в алгоритме симспелл все операции равнозначны, так как завязаны на одних только удалениях. Возможно поэтому и происходят всякие казусы типа испаравления 'какт' на 'как от', вместо 'как-то' или 'как'. \n",
        "То есть, основная причина падения качества - добавление словарей с изуродованными нграммами, что хорошо, для исправления некоторых ошибок внутри словосочетаний, но, в целом, негативно влияет на общее качество.   "
      ]
    }
  ]
}